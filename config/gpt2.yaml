# model config
model:
    name: gpt2
    model_path: ./gpt2-xl

# data config
train_datasets:
    name: unify_balanced

# training config
num_train_epochs: 100
bf16: True
per_device_train_batch_size: 36
per_device_eval_batch_size: 0
gradient_accumulation_steps: 1
evaluation_strategy: "no"
eval_steps: 1000
save_strategy: "steps"
save_steps: 1000
save_total_limit: 5
learning_rate: 0.0001
weight_decay: 0.01
warmup_ratio: 0.
lr_scheduler_type: "cosine"
logging_strategy: "steps"
logging_steps: 1
fsdp: False
fsdp_config:
  fsdp_transformer_layer_cls_to_wrap: False
tf32: True
model_max_length: 1024
gradient_checkpointing: True
train_on_inputs: False
deepspeed: ./config/ds_config.json

output_dir: ./output
output_root: ./logs/
seed: 123
